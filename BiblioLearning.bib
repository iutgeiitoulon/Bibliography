
@INBOOK{2019-Classification_Algorithm_Validation,
	author={A. K. {Nandi} and H. {Ahmed}},
	booktitle={Condition Monitoring with Vibration Signals: Compressive Sampling and Learning Algorithms for Rotating Machines},
	title={Classification Algorithm Validation},
	year={2019},
	volume={},
	number={},
	pages={307-319},
	keywords={Training;Testing;Data models;Vibrations;Classification algorithms;Task analysis;Condition monitoring},
	doi={10.1002/9781119544678.ch15},
	ISSN={null},
	publisher={IEEE},
	isbn={null},
	url={https://ieeexplore.ieee.org/document/8958927},}

@unpublished{ng2012cs229,
	added-at = {2016-11-24T22:48:07.000+0100},
	author = {Ng, Andrew},
	biburl = {https://www.bibsonomy.org/bibtex/24abdb571b427805f7bff139dbab7da85/nosebrain},
	interhash = {1895c04e83b69104b04117b567babedd},
	intrahash = {4abdb571b427805f7bff139dbab7da85},
	keywords = {learning logistic regression supervised},
	timestamp = {2016-11-24T22:48:07.000+0100},
	title = {CS229 Lecture notes - Supervised learning},
	year = 2012
}

@article{2015-ImprovingDistributionalSimilarityWithLessonsLearned,
	title = {Improving Distributional Similarity with Lessons Learned from Word Embeddings},
	author = {O. Levy and Y. Goldberg and I. Dagan},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {3},
	year = {2015},
	url = {https://www.aclweb.org/anthology/Q15-1016},
	doi = {10.1162/tacl_a_00134},
	pages = {211--225},
}

@article{2006-ArtificialNeuralNetworksVSLDA,
	title = {Artificial neural network vs linear discriminant analysis in credit ratings forecast: A comparative study of prediction performances},
	author = {K. Kumar and S. Bhattacharya},
	journal = {Review of Accounting and Finance},
	volume = {5},
	number = {3},
	year = {2006},
	pages = {216-227},
}

@article{1993-LDAvsANN,
	author = {Yoon, Youngohc and Swales, George and Margavio, Thomas},
	year = {1993},
	month = {01},
	pages = {51-60},
	title = {A Comparison of Discriminant Analysis Versus Artificial Neural Networks},
	volume = {44},
	journal = {Journal of The Operational Research Society - J OPER RES SOC},
	doi = {10.1057/jors.1993.6}
}

@article{1997-ClassicalvsANN,
	author = {Newey, V},
	year = {1997},
	month = {08},
	pages = {5-8},
	title = {Classical versus artificial neural network analysis},
	volume = {10},
	journal = {Ultrasound in obstetrics \& gynecology : the official journal of the International Society of Ultrasound in Obstetrics and Gynecology},
	doi = {10.1046/j.1469-0705.1997.10010005.x}
}

@article{2012-LDAvsANN,
	author = {Castro, M.C.F.},
	year = {2012},
	month = {01},
	pages = {351-355},
	title = {Linear discriminant analysis versus artificial neural network as classifiers for elbow angular position recognition purposes},
	journal = {BIOSIGNALS 2012 - Proceedings of the International Conference on Bio-Inspired Systems and Signal Processing}
}

@ARTICLE{2017-EmbeddedDeepNeuralNetworkProcessing,
	author={M. {Verhelst} and B. {Moons}},
	journal={IEEE Solid-State Circuits Magazine},
	title={Embedded Deep Neural Network Processing: Algorithmic and Processor Techniques Bring Deep Learning to IoT and Edge Devices},
	year={2017},
	volume={9},
	number={4},
	pages={55-65},
	abstract={Deep learning has recently become immensely popular for image recognition, as well as for other recognition and pattern matching tasks in, e.g., speech processing, natural language processing, and so forth. The online evaluation of deep neural networks, however, comes with significant computational complexity, making it, until recently, feasible only on power-hungry server platforms in the cloud. In recent years, we see an emerging trend toward embedded processing of deep learning networks in edge devices: mobiles, wearables, and Internet of Things (IoT) nodes. This would enable us to analyze data locally in real time, which is not only favorable in terms of latency but also mitigates privacy issues. Yet evaluating the powerful but large deep neural networks with power budgets in the milliwatt or even microwatt range requires a significant improvement in processing energy efficiency.},
	keywords={image matching;Internet of Things;learning (artificial intelligence);neural nets;embedded deep neural network;algorithmic techniques;processor techniques;deep learning;IoT;edge devices;image recognition;pattern matching;Feature extraction;Biological neural networks;Training data;Machine learning;Image recognition;Tutorials},
	doi={10.1109/MSSC.2017.2745818},
	ISSN={},
	month={Fall},}

@ARTICLE{Bianchi2019,
	author={V. {Bianchi} and M. {Bassoli} and G. {Lombardo} and P. {Fornacciari} and M. {Mordonini} and I. {De Munari}},
	journal={IEEE Internet of Things Journal},
	title={IoT Wearable Sensor and Deep Learning: An Integrated Approach for Personalized Human Activity Recognition in a Smart Home Environment},
	year={2019},
	volume={6},
	number={5},
	pages={8553-8562},
	keywords={assisted living;biomedical equipment;cloud computing;convolutional neural nets;geriatrics;home computing;image recognition;Internet of Things;learning (artificial intelligence);patient diagnosis;patient monitoring;sensors;wearable computers;wireless LAN;common home router;convolutional neural network network;daily activity monitor;IoT wearable sensor;personalized human activity recognition;smart home environment;continuous monitoring;human behaviors;ambient assisted living;medical diagnosis;elderly care;innovative HAR system;wearable devices;deep learning techniques;inertial measurement unit;Wi-Fi section;AAL;well-being management;rehabilitation;entertainment;surveillance;IMU;cloud service;Internet;CNN;embedded devices;low-cost devices;Wearable sensors;Activity recognition;Deep learning;Monitoring;Feature extraction;Internet of Things;Cloud computing;Activity recognition;Internet of Things (IoT);machine learning;wearable sensor},
	doi={10.1109/JIOT.2019.2920283},
	ISSN={2372-2541},
	month={Oct},}

@misc{2020-Interpretable_Super_Resolution ,
	title={Interpretable Super-Resolution via a Learned Time-Series Representation},
	author={Randall Balestriero and Herve Glotin and Richard G. Baraniuk},
	year={2020},
	eprint={2006.07713},
	archivePrefix={arXiv},
	primaryClass={eess.SP}
}

@misc{2019-Online_K-means_Clustering,
	title={Online k-means Clustering},
	author={Vincent Cohen-Addad and Benjamin Guedj and Varun Kanade and Guy Rom},
	year={2019},
	eprint={1909.06861},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@inproceedings{2004-MLConf-K-means_clustering_via_principal_component_analysis,
  title={K-means clustering via principal component analysis},
  author={Ding, Chris and He, Xiaofeng},
  booktitle={Proceedings of the twenty-first international conference on Machine learning},
  pages={29},
  year={2004}
}

@book{2020-Book-Joshi-Machine_Learning_and_Artificial_Intelligence,
	title={Machine Learning and Artificial Intelligence},
	author={Ameet V Joshi},
	isbn={978-3-030-26622-6},
	url={https://www.springer.com/gp/book/9783030266219},
	year={2020},
	publisher={Springer International Publishing}
}

@article{2016-Nature-Mastering_the_Game_of_Go_with_Deep_Neural_Networks_and_Tree_Search,
	added-at = {2016-03-11T14:36:05.000+0100},
	author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	biburl = {https://www.bibsonomy.org/bibtex/29e987f58d895c490144693139cbc90c7/ytyoun},
	doi = {10.1038/nature16961},
	interhash = {48430c7891aaf9fe2582faa8f5d076c1},
	intrahash = {9e987f58d895c490144693139cbc90c7},
	journal = {Nature},
	keywords = {baduk go google},
	month = jan,
	number = 7587,
	pages = {484--489},
	publisher = {Nature Publishing Group},
	timestamp = {2016-03-11T14:37:40.000+0100},
	title = {Mastering the Game of {Go} with Deep Neural Networks and Tree Search},
	volume = 529,
	year = 2016
}

@article{1958-Psychological_Review-The_Perceptron:_A_Probabilistic_Model_For_Information_Storage_And_Organization_In_The_Brain,
	author = {Franck Rosenblatt},
	doi = {10.1037/h0042519},
	journal = {Psychological Review},
	title = {The Perceptron: A Probabilistic Model For Information Storage And Organization In The Brain},
	year = 1958
}

@book{1969-MIT-Perceptrons:_An_Introduction_to_Computational_Geometry,
	added-at = {2008-05-16T13:57:01.000+0200},
	address = {Cambridge, MA, USA},
	author = {Minsky, Marvin and Papert, Seymour},
	biburl = {https://www.bibsonomy.org/bibtex/206a5a6751b3e61408455fca2ed8d87fc/sb3000},
	description = {: mf : blob : Â» bibtex},
	interhash = {d80d4948a422623047f1b800272c0389},
	intrahash = {06a5a6751b3e61408455fca2ed8d87fc},
	keywords = {linear-classification neural-networks seminal},
	publisher = {MIT Press},
	timestamp = {2008-05-16T13:57:02.000+0200},
	title = {Perceptrons: An Introduction to Computational Geometry},
	year = 1969
}

@phdthesis{1974-PhD_Thesis-Beyond_Regression:_New_Tools_for_Prediction_and_Analysis_in_the_Behavioral_Sciences,
	added-at = {2008-02-26T11:58:58.000+0100},
	author = {Werbos, P. J.},
	biburl = {https://www.bibsonomy.org/bibtex/2b0644d7aa84be0df0f198d586d341843/schaul},
	citeulike-article-id = {2381655},
	description = {idsia},
	interhash = {4165e2708a0468e89f8305f21ee2c711},
	intrahash = {b0644d7aa84be0df0f198d586d341843},
	keywords = {juergen},
	priority = {2},
	school = {Harvard University},
	timestamp = {2008-02-26T11:59:46.000+0100},
	title = {Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences},
	year = 1974
}

@INPROCEEDINGS{1988-IEEE_ICNN-Backpropagation:_past_and_future,  author={ {Werbos}},  booktitle={IEEE 1988 International Conference on Neural Networks},   title={Backpropagation: past and future},   year={1988},  volume={},  number={},  pages={343-353 vol.1},}

@Inbook{2012-Springer-Stochastic_Gradient_Descent_Tricks,
	author="Bottou, L{\'e}on",
	title="Stochastic Gradient Descent Tricks",
	bookTitle="Neural Networks: Tricks of the Trade: Second Edition",
	year="2012",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="421--436",
	abstract="Chapter 1 strongly advocates the stochastic back-propagation method to train neural networks. This is in fact an instance of a more general technique called stochastic gradient descent (SGD). This chapter provides background material, explains why SGD is a good learning algorithm when the training set is large, and provides useful recommendations.",
	isbn="978-3-642-35289-8",
	doi="10.1007/978-3-642-35289-8_25",
	url="https://doi.org/10.1007/978-3-642-35289-8_25"
}

@book{1996-Book-Numerical_Methods_for_Unconstrained_Optimization_and_Nonlinear_Equations,
	author = {Dennis, J. E. and Schnabel, Robert B.},
	title = {Numerical Methods for Unconstrained Optimization and Nonlinear Equations (Classics in Applied Mathematics, 16)},
	year = {1996},
	isbn = {0898713641},
	publisher = {Soc for Industrial and Applied Math}
}

@article{2016-CoRR-An_overview_of_gradient_descent_optimization_algorithms,
	author    = {Sebastian Ruder},
	title     = {An overview of gradient descent optimization algorithms},
	journal   = {CoRR},
	volume    = {abs/1609.04747},
	year      = {2016},
	url       = {http://arxiv.org/abs/1609.04747},
	archivePrefix = {arXiv},
	eprint    = {1609.04747},
	timestamp = {Mon, 13 Aug 2018 16:48:10 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/Ruder16.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{2014-arXiv-Adam_A_Method_for_Stochastic_Optimization,
	title={Adam: A Method for Stochastic Optimization},
	author={Diederik P. Kingma and Jimmy Lei Ba},
	year={2014},
	eprint={1412.6980},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	journal={International Conference on Learning Representations}
}

@article{2003-Pattern_Recognition-The_global_k-means_clustering_algorithm,
	title = "The global k-means clustering algorithm",
	journal = "Pattern Recognition",
	volume = "36",
	number = "2",
	pages = "451 - 461",
	year = "2003",
	note = "Biometrics",
	issn = "0031-3203",
	doi = "https://doi.org/10.1016/S0031-3203(02)00060-2",
	url = "http://www.sciencedirect.com/science/article/pii/S0031320302000602",
	author = "Aristidis Likas, Nikos Vlassis and Jakob J. Verbeek",
	keywords = "Clustering, -Means algorithm, Global optimization, - Trees, Data mining",
	abstract = "We present the global k-means algorithm which is an incremental approach to clustering that dynamically adds one cluster center at a time through a deterministic global search procedure consisting of N (with N being the size of the data set) executions of the k-means algorithm from suitable initial positions. We also propose modifications of the method to reduce the computational load without significantly affecting solution quality. The proposed clustering methods are tested on well-known data sets and they compare favorably to the k-means algorithm with random restarts."
}

@ARTICLE{2002-ToPAaMI_An_efficient_k-means_clustering_algorithm_analysis_and_implementation,  
	author={T. {Kanungo} and D. M. {Mount} and N. S. {Netanyahu} and C. D. {Piatko} and R. {Silverman} and A. Y. {Wu}},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},   
	title={An efficient k-means clustering algorithm: analysis and implementation},   
	year={2002},  
	volume={24},  
	number={7},  
	pages={881-892},
}

 @InProceedings{2012-JMLR-Autoencoders_Unsupervised_Learning_and_Deep_Architectures, 
 	title = {Autoencoders, Unsupervised Learning, and Deep Architectures}, 
 	author = {Pierre Baldi}, 
 	pages = {37--49}, 
 	year = {2012}, 
 	editor = {Isabelle Guyon and Gideon Dror and Vincent Lemaire and Graham Taylor and Daniel Silver}, 
 	volume = {27}, 
 	series = {Proceedings of Machine Learning Research}, 
 	address = {Bellevue, Washington, USA}, 
 	month = {02 Jul}, 
 	publisher = {JMLR Workshop and Conference Proceedings}, 
 	pdf = {http://proceedings.mlr.press/v27/baldi12a/baldi12a.pdf}, 
 	url = {http://proceedings.mlr.press/v27/baldi12a.html}, 
 	abstract = {Autoencoders play a fundamental role in unsupervised learning and in deep architectures for transfer learning and other tasks. In spite of their fundamental role, only linear autoencoders over the real numbers have been solved analytically. Here we present a general mathematical framework for the study of both linear and non-linear autoencoders. The framework allows one to derive an analytical treatment for the most non-linear autoencoder, the Boolean autoencoder. Learning in the Boolean autoencoder is equivalent to a clustering problem that can be solved in polynomial time when the number of clusters is small and becomes NP complete when the number of clusters is large. The framework sheds light on the different kinds of autoencoders, their learning complexity, their horizontal and vertical composability in deep architectures, their critical points, and their fundamental connections to clustering, Hebbian learning, and information theory.} 
 } 


@article{2015-Nature-Deep_Learning,
	title = "Deep learning",
	abstract = "Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.",
	author = "Yann Lecun and Yoshua Bengio and Geoffrey Hinton",
	year = "2015",
	month = may,
	day = "27",
	doi = "10.1038/nature14539",
	language = "English (US)",
	volume = "521",
	pages = "436--444",
	journal = "Nature Cell Biology",
	issn = "1465-7392",
	publisher = "Nature Publishing Group",
	number = "7553",	
}

@article{2014-Article-Deep_Learning_Methods_and_Applications,
	author = {Deng, Li and Yu, Dong},
	title = {Deep Learning: Methods and Applications},
	year = {2014},
	issue_date = {June 2014},
	publisher = {Now Publishers Inc.},
	address = {Hanover, MA, USA},
	volume = {7},
	number = {3â4},
	issn = {1932-8346},
	url = {https://doi.org/10.1561/2000000039},
	doi = {10.1561/2000000039},
	abstract = {This monograph provides an overview of general deep learning methodology and its applications to a variety of signal and information processing tasks. The application areas are chosen with the following three criteria in mind: (1) expertise or knowledge of the authors; (2) the application areas that have already been transformed by the successful use of deep learning technology, such as speech recognition and computer vision; and (3) the application areas that have the potential to be impacted significantly by deep learning and that have been experiencing research growth, including natural language and text processing, information retrieval, and multimodal information processing empowered by multi-task deep learning.},
	journal = {Found. Trends Signal Process.},
	month = jun,
	pages = {197â387},
	numpages = {191},
	keywords = {Autoencoders, Unsupervised learning, Hybrid deep networks, Language models, Multi-modal processing, Deep neural networks, Multi-task learning, Deep stacking networks, Machine learning, Deep learning, Object recognition, Natural language processing, Computer vision, Supervised learning, Artificial intelligence, Neural networks}
}
@article{2006-Neural_Computation-A_Fast_Learning_Algorithm_for_Deep_Belief_Nets,
	author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee-Whye},
	title = {A Fast Learning Algorithm for Deep Belief Nets},
	journal = {Neural Computation},
	volume = {18},
	number = {7},
	pages = {1527-1554},
	year = {2006},
	doi = {10.1162/neco.2006.18.7.1527},
	note ={PMID: 16764513},
	
	URL = { 
	https://doi.org/10.1162/neco.2006.18.7.1527
	
	},
	eprint = { 
	https://doi.org/10.1162/neco.2006.18.7.1527
	
	}
	,
	abstract = { We show how to use âcomplementary priorsâ to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind. }
}

@inbook{1998-MIT-Convolutional_Networks_for_Images_Speech_and_Time_Series,
	author = {LeCun, Yann and Bengio, Yoshua},
	title = {Convolutional Networks for Images, Speech, and Time Series},
	year = {1998},
	isbn = {0262511029},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
	booktitle = {The Handbook of Brain Theory and Neural Networks},
	pages = {255â258},
	numpages = {4}
}

@incollection{2012-Curran-ImageNet_Classification_with_Deep_Convolutional_Neural_Networks,
	title = {ImageNet Classification with Deep Convolutional Neural Networks},
	author = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
	booktitle = {Advances in Neural Information Processing Systems 25},
	editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
	pages = {1097--1105},
	year = {2012},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}

@misc{2014-arXiv-Do_Deep_Nets_Really_Need_to_be_Deep,
      title={Do Deep Nets Really Need to be Deep?}, 
      author={Lei Jimmy Ba and Rich Caruana},
      year={2014},
      eprint={1312.6184},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{2013-arXiv-Big_Neural_Networks_Waste_Capacity,
      title={Big Neural Networks Waste Capacity}, 
      author={Yann N. Dauphin and Yoshua Bengio},
      year={2013},
      eprint={1301.3583},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{2012-arXiv-Improving_neural_networks_by_preventing_co_adaptation_of_feature_detectors,
      title={Improving neural networks by preventing co-adaptation of feature detectors}, 
      author={Geoffrey E. Hinton and Nitish Srivastava and Alex Krizhevsky and Ilya Sutskever and Ruslan R. Salakhutdinov},
      year={2012},
      eprint={1207.0580},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@inproceedings{2010-ICML-Rectified_linear_units_improve_restricted_boltzmann_machines,
		author = {Nair, Vinod and Hinton, Geoffrey E.},
		title = {Rectified Linear Units Improve Restricted Boltzmann Machines},
		year = {2010},
		isbn = {9781605589077},
		publisher = {Omnipress},
		address = {Madison, WI, USA},
		abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these "Stepped Sigmoid Units" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
		booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
		pages = {807â814},
		numpages = {8},
		location = {Haifa, Israel},
		series = {ICML'10}
}




