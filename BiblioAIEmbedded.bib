
@INPROCEEDINGS{2018_MESA_Low-Power_Wake-Up_System_based_on_Frequency_Analysis,
	author={Fourniol, Manon and Gies, Valentin and Barchasz, Valentin and Kussener, Edith and Barthelemy, Herv\'e and Vauch\'e, Remy and Glotin, Herv\'e},
	booktitle={2018 14th IEEE/ASME International Conference on Mechatronic and Embedded Systems and Applications (MESA)}, 
	title={Low-Power Wake-Up System based on Frequency Analysis for Environmental Internet of Things}, 
	year={2018},
	volume={},
	number={},
	pages={1-6},}


@INPROCEEDINGS{2019_ISOCC_AI_32_TFLOPS_Autonomous_Driving_Processor,
	author={Y. {Kwon} and Y. P. {Cho} and J. {Yang} and J. {Chung} and K. {Shin} and J. {Han} and C. {Kim} and C. {Lyuh} and H. {Kim} and I. {Jeon} and M. {Choi}},
	booktitle={2019 International SoC Design Conference (ISOCC)}, 
	title={AI 32TFLOPS Autonomous Driving Processor on AI-Ware with Adaptive Power Saving}, 
	year={2019},
	volume={},
	number={},
	pages={174-175},}

@INPROCEEDINGS{2019_ICCS_Low_Power_AI_Processor_Autonomous_Mobile_Robot,
	author={A. {Anilkumar} and A. {P} and A. S. {Nalluri} and A. {Santhanam} and P. D. {Kedhari}},
	booktitle={2019 International Conference on Intelligent Computing and Control Systems (ICCS)}, 
	title={A low power Artificial Intelligence Processor for Autonomous Mobile Robots}, 
	year={2019},
	volume={},
	number={},
	pages={495-499},}

@INPROCEEDINGS{2016_COOL_CHIPS_1_1mW_AI_processor,
	author={Y. {Kim} and D. {Shin} and J. {Lee} and H. {Yoo}},
	booktitle={2016 IEEE Symposium in Low-Power and High-Speed Chips (COOL CHIPS XIX)}, 
	title={A 1.1 mW 32-thread artificial intelligence processor with 3-level transposition table and on-chip PVT compensation for autonomous mobile robots}, 
	year={2016},
	volume={},
	number={},
	pages={1-2},}

@INPROCEEDINGS{2011_AIMSEC_Low_Power_Performance_Achievement_In_Embedded_System,
	author={ {Xiaoqiong Zuo} and  {Liu Yaxian}},
	booktitle={2011 2nd International Conference on Artificial Intelligence, Management Science and Electronic Commerce (AIMSEC)}, 
	title={Low power performance achievement in embedded system}, 
	year={2011},
	volume={},
	number={},
	pages={4655-4658},}

@INPROCEEDINGS{2019_GIOTS_Developing_IoT_Devices_Empowered_byAI,
	author={V. {Prutyanov} and N. {Melentev} and D. {Lopatkin} and A. {Menshchikov} and A. {Somov}},
	booktitle={2019 Global IoT Summit (GIoTS)}, 
	title={Developing IoT Devices Empowered by Artificial Intelligence: Experimental Study}, 
	year={2019},
	volume={},
	number={},
	pages={1-6},}

@INPROCEEDINGS{2019_ICCS_Low_Power_Intelligence_Processor_Autonomous_Mobile_Robots,
	author={A. {Anilkumar} and A. {P} and A. S. {Nalluri} and A. {Santhanam} and P. D. {Kedhari}},
	booktitle={2019 International Conference on Intelligent Computing and Control Systems (ICCS)}, 
	title={A low power Artificial Intelligence Processor for Autonomous Mobile Robots}, 
	year={2019},
	volume={},
	number={},
	pages={495-499},}
@INPROCEEDINGS{2013_S3S_Low_power_false_positive_tolerant_event_detector_for_seismic_sensors,
	
	author={U. {Antao} and A. {Dibazar} and J. {Choma} and T. {Berger}},
	
	booktitle={2013 IEEE SOI-3D-Subthreshold Microelectronics Technology Unified Conference (S3S)}, 
	
	title={Low power false positive tolerant event detector for seismic sensors}, 
	
	year={2013},
	
	volume={},
	
	number={},
	
	pages={1-2},}

@INPROCEEDINGS{2014_ICCE_Deep_learning_for_real_time_robust_facial_expression_recognition_on_a_smartphone,  
	author={I. {Song} and H. {Kim} and P. B. {Jeon}},  booktitle={2014 IEEE International Conference on Consumer Electronics (ICCE)},   
	title={Deep learning for real-time robust facial expression recognition on a smartphone},   year={2014},  
	volume={},  
	number={},  
	pages={564-567},}

@inproceedings{2015_AAAI_Deep_Convolutional_Neural_Networks_on_Multichannel_Time_Series_for_Human_Activity_Recognition,
	author = {Yang, Jian Bo and Nguyen, Minh Nhut and San, Phyo Phyo and Li, Xiao Li and Krishnaswamy, Shonali},
	title = {Deep Convolutional Neural Networks on Multichannel Time Series for Human Activity Recognition},
	year = {2015},
	isbn = {9781577357384},
	publisher = {AAAI Press},
	booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
	pages = {3995–4001},
	numpages = {7},
	location = {Buenos Aires, Argentina},
	series = {IJCAI’15}
}
@ARTICLE{1985_BME_A_Real_Time_QRS_Detection_Algorithm,  
	author={J. {Pan} and W. J. {Tompkins}},  journal={IEEE Transactions on Biomedical Engineering},   
	title={A Real-Time QRS Detection Algorithm},   year={1985},  
	volume={BME-32},  
	number={3},  
	pages={230-236},}
@ARTICLE{2019_IOT_IoT_Wearable_Sensor_and_Deep_Learning_An_Integrated_Approach_for_Personalized_Human_Activity_Recognition_in_a_Smart_Home_Environment,  
	author={V. {Bianchi} and M. {Bassoli} and G. {Lombardo} and P. {Fornacciari} and M. {Mordonini} and I. {De Munari}},  
	journal={IEEE Internet of Things Journal},   title={IoT Wearable Sensor and Deep Learning: An Integrated Approach for Personalized Human Activity Recognition in a Smart Home Environment},   year={2019},  
	volume={6},  
	number={5},  
	pages={8553-8562},}

@INBOOK{2016_IEEE_ARM-Floating-Point-Unit(FPU),  author={Y. {Bai}},  booktitle={Practical Microcontroller Engineering with ARM­ Technology},   title={ARM® Floating Point Unit (FPU)},   year={2016},  volume={},  number={},  pages={927-950},  doi={10.1002/9781119058397.ch11}}

@INPROCEEDINGS{2019_ICASSP_Convolutional_Neural_Network_on_Embedded_Platform_for_People_Presence_Detection_in_Low_Resolution_Thermal_Images,  
	author={G. {Cerutti} and R. {Prasad} and E. {Farella}},  
	booktitle={ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},   
	title={Convolutional Neural Network on Embedded Platform for People Presence Detection in Low Resolution Thermal Images},   
	year={2019},  
	volume={},  
	number={},  
	pages={7610-7614},}

@INPROCEEDINGS{2018_ICCAD_Enabling_Deep_Learning_at_the_LoT_Edge,  
	author={L. {Lai} and N. {Suda}},  
	booktitle={2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)},  
	title={Enabling Deep Learning at the LoT Edge},   year={2018},  
	volume={},  
	number={},  
	pages={1-6},}

@INPROCEEDINGS{2017_IWASI_DeepEmote_Towards_multi-layer_neural_networks_in_a_low_power_wearable_multi-sensors_bracelet,  
	author={M. {Magno} and M. {Pritz} and P. {Mayer} and L. {Benini}},  
	booktitle={2017 7th IEEE International Workshop on Advances in Sensors and Interfaces (IWASI)}, 
	title={DeepEmote: Towards multi-layer neural networks in a low power wearable multi-sensors bracelet},   
	year={2017},  
	volume={},  
	number={},  
	pages={32-37},}

@INPROCEEDINGS{2019_WF-IoT_FANNCortexM_An_Open_Source_Toolkit_for_Deployment_of_Multi-layer_Neural_Networks_on_ARM_Cortex-M_Family_Microcontrollers_Performance_Analysis_with_Stress_Detection,  
	author={M. {Magno} and L. {Cavigelli} and P. {Mayer} and F. v. {Hagen} and L. {Benini}},  
	booktitle={2019 IEEE 5th World Forum on Internet of Things (WF-IoT)},   
	title={FANNCortexM: An Open Source Toolkit for Deployment of Multi-layer Neural Networks on ARM Cortex-M Family Microcontrollers : Performance Analysis with Stress Detection},   
	year={2019},  
	volume={},  
	number={},  
	pages={793-798},}

@INPROCEEDINGS{2018_DSD_Embedded_Real-Time_Fall_Detection_with_Deep_Learning_on_Wearable_Devices,  
	author={E. {Torti} and A. {Fontanella} and M. {Musci} and N. {Blago} and D. {Pau} and F. {Leporati} and M. {Piastra}},  
	booktitle={2018 21st Euromicro Conference on Digital System Design (DSD)},   
	title={Embedded Real-Time Fall Detection with Deep Learning on Wearable Devices},   
	year={2018},  
	volume={},  
	number={},  
	pages={405-412},}

@article{2019_SD_Embedding_Recurrent_Neural_Networks_in_Wearable_Systems_for_Real-Time_Fall_Detection,
	journal = {Microprocessors and Microsystems},
	title = {Embedding Recurrent Neural Networks in Wearable Systems for Real-Time Fall Detection},
	journal = {Microprocessors and Microsystems},
	volume = {71},
	pages = {102895},
	year = {2019},
	issn = {0141-9331},
	doi = {https://doi.org/10.1016/j.micpro.2019.102895},
	url = {http://www.sciencedirect.com/science/article/pii/S0141933119300584},
	author = {Emanuele Torti and Alessandro Fontanella and Mirto Musci and Nicola Blago and Danilo Pau and Francesco Leporati and Marco Piastra},
}

@article{2018_SD_Deep_learning_algorithms_for_human_activity_recognition_using_mobile_and_wearable_sensor_networks_State_of_the_art_and_research_challenges,
	title = {Deep learning algorithms for human activity recognition using mobile and wearable sensor networks: State of the art and research challenges},
	journal = {Expert Systems with Applications},
	volume = {105},
	pages = {233 - 261},
	year = {2018},
	issn = {0957-4174},
	doi = {https://doi.org/10.1016/j.eswa.2018.03.056},
	url = {http://www.sciencedirect.com/science/article/pii/S0957417418302136},
	author = {Henry Friday Nweke and Ying Wah Teh and Mohammed Ali Al-garadi and Uzoma Rita Alo},	
}

@INPROCEEDINGS{2020_I2MTC_Embedded_Learning_for_Smart_FES,
	title= {Embedded Learning for Smart Functional Electrical Stimulation},
	booktitle = {2020 IEEE International Instrumentation and Measurement Technology Conference},
	author = {Marzetti, Sebasti\'an and Gies, Valentin and Barchasz, Valentin and Barth\'elemy, Herv\'e and Glotin, Herv\'e and Kussener, Edith and Vauch\'e, Remy},
	year = {2020},
	volume = {},
	pages = {},
	number={},
}

@INPROCEEDINGS{2017_ASRU_An_embedded_segmental_K-means_model_for_unsupervised_segmentation_and_clustering_of_speech,  
	author={H. {Kamper} and K. {Livescu} and S. {Goldwater}},  
	booktitle={2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},   
	title={An embedded segmental K-means model for unsupervised segmentation and clustering of speech},   
	year={2017},  
	volume={},  
	number={},  
	pages={719-726},}

@INPROCEEDINGS{2019_DATE_Enabling_Energy-Efficient_Unsupervised_Monocular_Depth_Estimation_on_ARMv7-Based_Platforms,  
	author={V. {Peluso} and A. {Cipolletta} and A. {Calimera} and M. {Poggi} and F. {Tosi} and S. {Mattoccia}},  
	booktitle={2019 Design, Automation   Test in Europe Conference   Exhibition (DATE)},  
	title={Enabling Energy-Efficient Unsupervised Monocular Depth Estimation on ARMv7-Based Platforms},   
	year={2019},  
	volume={},  
	number={},  
	pages={1703-1708},}

@ARTICLE{2011_IEEECirc&Syst_Power-Efficient_Hardware_Architecture_of_K-Means_Clustering_With_Bayesian-Information-Criterion_Processor_for_Multimedia_Processing_Applications,  
	author={T. {Chen} and C. {Sun} and H. {Su} and S. {Chien} and D. {Deguchi} and I. {Ide} and H. {Murase}},  
	journal={IEEE Journal on Emerging and Selected Topics in Circuits and Systems},   
	title={Power-Efficient Hardware Architecture of K-Means Clustering With Bayesian-Information-Criterion Processor for Multimedia Processing Applications},   
	year={2011},  
	volume={1},  
	number={3},  
	pages={357-368},}

@article{2015_SD_Recognizing_upper_limb_movements_with_wrist_worn_inertial_sensors_using_k-means_clustering_classification,
	title = {Recognizing upper limb movements with wrist worn inertial sensors using k-means clustering classification},
	journal = {Human Movement Science},
	volume = {40},
	pages = {59 - 76},
	year = {2015},
	issn = {0167-9457},
	doi = {https://doi.org/10.1016/j.humov.2014.11.013},
	url = {http://www.sciencedirect.com/science/article/pii/S0167945714002115},
	author = {Dwaipayan Biswas and Andy Cranny and Nayaab Gupta and Koushik Maharatna and Josy Achner and Jasmin Klemke and Michael Jöbges and Steffen Ortmann},
	keywords = {2330, 3380, Activities of daily living, Activity recognition, Inertial sensors, -Means clustering, Minimum distance classifier, Remote telehealth monitoring},
	abstract = {In this paper we present a methodology for recognizing three fundamental movements of the human forearm (extension, flexion and rotation) using pattern recognition applied to the data from a single wrist-worn, inertial sensor. We propose that this technique could be used as a clinical tool to assess rehabilitation progress in neurodegenerative pathologies such as stroke or cerebral palsy by tracking the number of times a patient performs specific arm movements (e.g. prescribed exercises) with their paretic arm throughout the day. We demonstrate this with healthy subjects and stroke patients in a simple proof of concept study in which these arm movements are detected during an archetypal activity of daily-living (ADL) – ‘making-a-cup-of-tea’. Data is collected from a tri-axial accelerometer and a tri-axial gyroscope located proximal to the wrist. In a training phase, movements are initially performed in a controlled environment which are represented by a ranked set of 30 time-domain features. Using a sequential forward selection technique, for each set of feature combinations three clusters are formed using k-means clustering followed by 10 runs of 10-fold cross validation on the training data to determine the best feature combinations. For the testing phase, movements performed during the ADL are associated with each cluster label using a minimum distance classifier in a multi-dimensional feature space, comprised of the best ranked features, using Euclidean or Mahalanobis distance as the metric. Experiments were performed with four healthy subjects and four stroke survivors and our results show that the proposed methodology can detect the three movements performed during the ADL with an overall average accuracy of 88% using the accelerometer data and 83% using the gyroscope data across all healthy subjects and arm movement types. The average accuracy across all stroke survivors was 70% using accelerometer data and 66% using gyroscope data. We also use a Linear Discriminant Analysis (LDA) classifier and a Support Vector Machine (SVM) classifier in association with the same set of features to detect the three arm movements and compare the results to demonstrate the effectiveness of our proposed methodology.}
}

@article{2011_SDExpertSystems_Rolling_element_bearing_fault_detection_in_industrial_environments_based_on_a_K-means_clustering_approach,
	title = "Rolling element bearing fault detection in industrial environments based on a K-means clustering approach",
	journal = "Expert Systems with Applications",
	volume = "38",
	number = "3",
	pages = "2888 - 2911",
	year = "2011",
	issn = "0957-4174",
	doi = "https://doi.org/10.1016/j.eswa.2010.08.083",
	url = "http://www.sciencedirect.com/science/article/pii/S0957417410008791",
	author = "C.T. Yiakopoulos and K.C. Gryllias and I.A. Antoniadis",
	keywords = "Condition monitoring, Fault detection, -means clustering, Vibration analysis, Rolling element bearings",
	abstract = "A K-means clustering approach is proposed for the automated diagnosis of defective rolling element bearings. Since K-means clustering is an unsupervised learning procedure, the method can be directly implemented to measured vibration data. Thus, the need for training the method with data measured on the specific machine under defective bearing conditions is eliminated. This fact consists the major advantage of the method, especially in industrial environments. Critical to the success of the method is the feature set used, which consists of a set of appropriately selected frequency-domain parameters, extracted both from the raw signal, as well as from the signal envelope, as a result of the engineering expertise, gained from the understanding of the physical behavior of defective rolling element bearings. Other advantages of the method are its ease of programming, simplicity and robustness. In order to overcome the sensitivity of the method to the choice of the initial cluster centers, the initial centers are selected using features extracted from simulated signals, resulting from a well established model for the dynamic behavior of defective rolling element bearings. Then, the method is implemented as a two-stage procedure. At the first step, the method decides whether a bearing fault exists or not. At the second step, the type of the defect (e.g. inner or outer race) is identified. The effectiveness of the method is tested in one literature established laboratory test case and in three different industrial test cases. Each test case includes successive measurements from bearings under different types of defects. In all cases, the method presents a 100% classification success. Contrarily, a K-means clustering approach, which is based on typical statistical time domain based features, presents an unstable classification behavior."
}

@INPROCEEDINGS{2014_INISTA_Bearing_fault_diagnosis_using_hybrid_genetic_algorithm_K-means_clustering,  
	author={M. M. {Ettefagh} and M. {Ghaemi} and M. {Yazdanian Asr}},  
	booktitle={2014 IEEE International Symposium on Innovations in Intelligent Systems and Applications (INISTA) Proceedings},   
	title={Bearing fault diagnosis using hybrid genetic algorithm K-means clustering},   
	year={2014},  
	volume={},  
	number={},  
	pages={84-89},}

@INPROCEEDINGS{2016_I2MTC_A_method_of_automatic_feature_extraction_from_massive_vibration_signals_of_machines,  
	author={F. {Jia} and Y. {Lei} and S. {Xing} and J. {Lin}},  
	booktitle={2016 IEEE International Instrumentation and Measurement Technology Conference Proceedings},   
	title={A method of automatic feature extraction from massive vibration signals of machines},   
	year={2016},  
	volume={},  
	number={},  
	pages={1-6},}

@inproceedings{2013_MechaScience_The_Improved_K-Means_Cluster_Analysis_on_Diagnosis_Data_Fusion_of_the_Aero-Engine,
	author = {Liu, Xiao Bo and Deng, Bei Bei and Shen, Liang Ni},
	title = {The Improved K-Means Cluster Analysis on Diagnosis Data Fusion of the Aero-Engine},
	year = {2013},
	month = {7},
	volume = {328},
	pages = {463--467},
	booktitle = {Mechanical Science and Engineering III},
	series = {Applied Mechanics and Materials},
	publisher = {Trans Tech Publications Ltd},
	doi = {10.4028/www.scientific.net/AMM.328.463},
	keywords = {Aero-Engine, Initial Clustering Center, Hierarchical Clustering Algorithm, Improved K-Means Clustering Algorithm},
	abstract = {Aiming at the problem about initial clustering center was randomly assigned in K-means clustering algorithm, the improved K-means clustering algorithm based on hierarchical clustering algorithm and K-means clustering algorithm was proposed in this paper. In the improved algorithm, first of all K was calculated by hierarchical clustering. When K was determined, K-means clustering was implemented. The results of the aero-engine vibration data clustering shown that not only the k value was to quickly and accurately determined, but also the number of clusters can be reduced and higher computing efficiency can be attained by the improved K-means clustering algorithm.}
}

@INPROCEEDINGS{2016_ICIS_Fault_behaviour_pattern_analysis_and_recognition,  
	author={G. K. {Durbhaka} and S. {Barani}},  
	booktitle={2016 International Conference on Information Science (ICIS)},   
	title={Fault behaviour pattern analysis and recognition},   
	year={2016},  
	volume={},  
	number={},  
	pages={191-193},}

@inbook{2010_AIAA_Independent_Component_Analysis_and_K-Means_Clustering_for_Damage_Detection_in_Structural_Systems,
	author = {Mohamed Elseifi},
	title = {Independent Component Analysis and K-Means Clustering for Damage Detection in Structural Systems},
	booktitle = {51st AIAA/ASME/ASCE/AHS/ASC Structures, Structural Dynamics, and Materials Conference},
	chapter = {},
	pages = {},
	doi = {10.2514/6.2010-2772},
	URL = {https://arc.aiaa.org/doi/abs/10.2514/6.2010-2772},
	eprint = {https://arc.aiaa.org/doi/pdf/10.2514/6.2010-2772},
}

@ARTICLE{2018-IEEE-Access-Benchmark_Analysis_of_Representative_Deep_Neural_Network,
	author={S. {Bianco} and R. {Cadene} and L. {Celona} and P. {Napoletano}},
	journal={IEEE Access}, 
	title={Benchmark Analysis of Representative Deep Neural Network Architectures}, 
	year={2018},
	volume={6},
	number={},
	pages={64270-64277},
	doi={10.1109/ACCESS.2018.2877890}}

@article{2019-Neural-Networks-Recent_advances_in_physical_reservoir_computing,
	title = "Recent advances in physical reservoir computing: A review",
	journal = "Neural Networks",
	volume = "115",
	pages = "100 - 123",
	year = "2019",
	issn = "0893-6080",
	doi = "https://doi.org/10.1016/j.neunet.2019.03.005",
	url = "http://www.sciencedirect.com/science/article/pii/S0893608019300784",
	author = "Gouhei Tanaka and Toshiyuki Yamane and Jean Benoit Héroux and Ryosho Nakane and Naoki Kanazawa and Seiji Takeda and Hidetoshi Numata and Daiju Nakano and Akira Hirose",
	keywords = "Neural networks, Machine learning, Reservoir computing, Nonlinear dynamical systems, Neuromorphic device"
}

@ARTICLE{2020-Convolution-Kernel-Operations-on-a-Two-Dimensional-Spin-Memristor-Cross-Array,
	author={Zhu {Saike} and Wang {Lidan} and Dong  {Zhekang} and Duan {Shukai}},
	journal={MDPI Sensors}, 
	title={Convolution Kernel Operations on a Two-Dimensional Spin Memristor Cross Array.}, 
	year={2020},
	volume={21},
	number={}
}

@ARTICLE{2020-A-Parasitic-Resistance-Adapted-Programming-Scheme-for-Memristor-Crossbar-Based-Neuromorphic-Computing-Systems,
	author={Ngoc {Truong}},
	journal={MDPI Materials}, 
	title={A Parasitic Resistance-Adapted Programming Scheme for Memristor Crossbar-Based Neuromorphic Computing Systems}, 
	year={2019},
	volume={12},
	number={}
}


@INPROCEEDINGS{2020-IoTAIS-Ultra-Low-Power-Always-on-Wake-Up-by-Pulse-Pattern-Adaptive-Recognition-for-Long-Term-Biodiversity-Monitoring,
	author={Marzetti, Sebasti\'an and Gies, Valentin and Barchasz, Valentin and Best, Paul and Paris, S\'ebastien  and Barth\'elemy, Herv\'e and Glotin, Herv\'e},
	booktitle={IEEE International Conference on Internet of Things and Intelligence System (IoTAIS)}, 
	title={Ultra Low-Power Always-on Wake-Up by Pulse Pattern Adaptive Recognition for Long Term Biodiversity Monitoring}, 
	year={2020}
}

@INPROCEEDINGS{2020-IoTAIS-Ultra-Low-Power-Embedded-Unsupervised-Learning-Smart-Sensor-for-Industrial-Fault-Detection,
	author={Gies, Valentin and Marzetti, Sebasti\'an and Barchasz, Valentin and Barth\'elemy, Herv\'e and Glotin, Herv\'e},
	booktitle={IEEE International Conference on Internet of Things and Intelligence System (IoTAIS)}, 
	title={Ultra-Low Power Embedded Unsupervised Learning Smart Sensor for Industrial Fault Detection}, 
	year={2020}
}

@ARTICLE{gies2021CasMagazine,
	author={Marzetti, Sebasti\'an and Gies, Valentin and Peyronnet, Pierre-Alexandre and Barth\'elemy, Florent and Barchasz, Valentin and Delaey-Langlois, Thomas 
	and Peloux, Daniel and Arlotto, Philippe and Barth\'elemy, Herv\'e },
	journal={IEEE CAS Magazine}, 
	title={Low Cost Artificial Ventilator Embedding Unsupervised Learning for Hardware Failure Detection (accepted)}, 
	year={2021},
}

@INPROCEEDINGS{2016_IEEE_ARM-Floating-Point-Unit(FPU),  
	author={Y. {Bai}},  
	booktitle={Practical Microcontroller Engineering with ARM­ Technology},   
	title={ARM® Floating Point Unit (FPU)},   
	year={2016},  
	volume={},  
	number={},  
	pages={927-950},  
	doi={10.1002/9781119058397.ch11}}

@misc{2017-Interspeech-Convolutional_Recurrent_Neural_Networks_for_Small_Footprint_Keyword_Spotting,
	title={Convolutional Recurrent Neural Networks for Small-Footprint Keyword Spotting}, 
	author={Sercan O. Arik and Markus Kliegl and Rewon Child and Joel Hestness and Andrew Gibiansky and Chris Fougner and Ryan Prenger and Adam Coates},
	year={2017},
	eprint={1703.05390},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@INPROCEEDINGS{2018-WF-IoT-Extending_the_battery_lifetime_of_wearable_sensors_with_embedded_machine_learning,  
	author={X. {Fafoutis} and L. {Marchegiani} and A. {Elsts} and J. {Pope} and R. {Piechocki} and I. {Craddock}},  
	booktitle={2018 IEEE 4th World Forum on Internet of Things (WF-IoT)},   
	title={Extending the battery lifetime of wearable sensors with embedded machine learning},   
	year={2018},  
	volume={},  
	number={},  
	pages={269-274},  
	doi={10.1109/WF-IoT.2018.8355116}
}

@article{2016-IEEE_SLT-Max-pooling_loss_training_of_long_short-term_memory_networks_for_small-footprint_keyword_spotting,
	title={Max-pooling loss training of long short-term memory networks for small-footprint keyword spotting},
	ISBN={9781509049035},
	url={http://dx.doi.org/10.1109/SLT.2016.7846306},
	DOI={10.1109/slt.2016.7846306},
	journal={2016 IEEE Spoken Language Technology Workshop (SLT)},
	publisher={IEEE},
	author={Sun, Ming and Raju, Anirudh and Tucker, George and Panchapagesan, Sankaran and Fu, Gengshen and Mandal, Arindam and Matsoukas, Spyros and Strom, Nikko and Vitaladevuni, Shiv},
	year={2016},
	month={Dec}
}
@inproceedings{2015-Interspeech-Convolutional_Neural_Networks_for_Small-Footprint_Keyword_Spotting,
	title	= {Convolutional Neural Networks for Small-Footprint Keyword Spotting},
	author	= {Tara Sainath and Carolina Parada},
	year	= {2015},
	booktitle	= {Interspeech}
}

@INPROCEEDINGS{2014-ICASSP-Small-footprint_keyword_spotting_using_deep_neural_networks,  
	author={G. {Chen} and C. {Parada} and G. {Heigold}},  
	booktitle={2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},   title={Small-footprint keyword spotting using deep neural networks},   year={2014},  volume={},  number={},  pages={4087-4091},  doi={10.1109/ICASSP.2014.6854370}}



@inproceedings{2003-SpikingNN-Vreeken,
	title={Spiking neural networks, an introduction},
	author={Jilles Vreeken},
	year={2003}
}

@article{2019-SD-Deep_learning_in_spiking_neural_networks,
	title = {Deep learning in spiking neural networks},
	journal = {Neural Networks},
	volume = {111},
	pages = {47-63},
	year = {2019},
	issn = {0893-6080},
	doi = {https://doi.org/10.1016/j.neunet.2018.12.002},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608018303332},
	author = {Amirhossein Tavanaei and Masoud Ghodrati and Saeed Reza Kheradpisheh and Timothée Masquelier and Anthony Maida},
	keywords = {Deep learning, Spiking neural network, Biological plausibility, Machine learning, Power-efficient architecture},
	abstract = {In recent years, deep learning has revolutionized the field of machine learning, for computer vision in particular. In this approach, a deep (multilayer) artificial neural network (ANN) is trained, most often in a supervised manner using backpropagation. Vast amounts of labeled training examples are required, but the resulting classification accuracy is truly impressive, sometimes outperforming humans. Neurons in an ANN are characterized by a single, static, continuous-valued activation. Yet biological neurons use discrete spikes to compute and transmit information, and the spike times, in addition to the spike rates, matter. Spiking neural networks (SNNs) are thus more biologically realistic than ANNs, and are arguably the only viable option if one wants to understand how the brain computes at the neuronal description level. The spikes of biological neurons are sparse in time and space, and event-driven. Combined with bio-plausible local learning rules, this makes it easier to build low-power, neuromorphic hardware for SNNs. However, training deep SNNs remains a challenge. Spiking neurons’ transfer function is usually non-differentiable, which prevents using backpropagation. Here we review recent supervised and unsupervised methods to train deep SNNs, and compare them in terms of accuracy and computational cost. The emerging picture is that SNNs still lag behind ANNs in terms of accuracy, but the gap is decreasing, and can even vanish on some tasks, while SNNs typically require many fewer operations and are the better candidates to process spatio-temporal data.}
}

@article{2009-Neural-Spiking_Neural_Networks,
	author = {Ghosh-Dastidar, Samanwoy and Adeli, Hojjat},
	year = {2009},
	month = {08},
	pages = {295-308},
	title = {Spiking Neural Networks.},
	volume = {19},
	journal = {Int. J. Neural Syst.},
	doi = {10.1142/S0129065709002002}
}

@article {1995-Science-Cracking_the_Neuronal_Code,
	author = {Ferster, David and Spruston, Nelson},
	title = {Cracking the Neuronal Code},
	volume = {270},
	number = {5237},
	pages = {756--757},
	year = {1995},
	doi = {10.1126/science.270.5237.756},
	publisher = {American Association for the Advancement of Science},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/270/5237/756},
	eprint = {https://science.sciencemag.org/content/270/5237/756.full.pdf},
	journal = {Science}
}

@article{2001-Neural_Networks-Spike_based_strategies_for_rapid_processing,
	title = {Spike-based strategies for rapid processing},
	journal = {Neural Networks},
	volume = {14},
	number = {6},
	pages = {715-725},
	year = {2001},
	issn = {0893-6080},
	doi = {https://doi.org/10.1016/S0893-6080(01)00083-1},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608001000831},
	author = {Simon Thorpe and Arnaud Delorme and Rufin {Van Rullen}},
	keywords = {Rank Order Coding, Latency, Rapid visual processing, Spikes, Retina, Information},
	abstract = {Most experimental and theoretical studies of brain function assume that neurons transmit information as a rate code, but recent studies on the speed of visual processing impose temporal constraints that appear incompatible with such a coding scheme. Other coding schemes that use the pattern of spikes across a population a neurons may be much more efficient. For example, since strongly activated neurons tend to fire first, one can use the order of firing as a code. We argue that Rank Order Coding is not only very efficient, but also easy to implement in biological hardware: neurons can be made sensitive to the order of activation of their inputs by including a feed-forward shunting inhibition mechanism that progressively desensitizes the neuronal population during a wave of afferent activity. In such a case, maximum activation will only be produced when the afferent inputs are activated in the order of their synaptic weights.}
}

@article{2001-Neural_Networks-Is_the_integrate_and_fire_model_good_enough_a_review,
	title = {Is the integrate-and-fire model good enough?—a review},
	journal = {Neural Networks},
	volume = {14},
	number = {6},
	pages = {955-975},
	year = {2001},
	issn = {0893-6080},
	doi = {https://doi.org/10.1016/S0893-6080(01)00074-0},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608001000740},
	author = {Jianfeng Feng},
	keywords = {The integrate-and-fire model, The Hodgkin-Huxley model, The Fitzhugh-Nagumo model, The IF-FHN model, Inhibition boosted firing},
	abstract = {We review some recent results on the behaviour of the integrate-and-fire (IF) model, the FitzHugh–Nagumo (FHN) model, a simplified version of the FHN (IF-FHN) model and the Hodgkin–Huxley (HH) model with correlated inputs. The effect of inhibitory inputs on the model behaviour is also taken into account. Here, inputs exclusively take the form of diffusion approximation and correlated inputs mean correlated synaptic inputs (2 The integrate-and-fire model and the Hodgkin–Huxley model, 3 Synaptic inputs). It is found that the IF and HH models respond to correlated inputs in totally opposite ways, but the IF-FHN model shows similar behaviour to the HH model. Increasing inhibitory input to single neuronal models, such as the FHN model and the HH model can sometimes increase their firing rates, which we termed inhibition-boosted firing (IBF). Using the IF model and the IF-FHN model, we theoretically explore how and when IBF can happen. The computational complexity of the IF-FHN model is very similar to the conventional IF model, but the former captures some interesting and essential features of biophysical models and could serve as a better model for spiking neuron computation.}
}

@article{2000-NeuralNetwork-Integrate_and_fire_Models_with_Nonlinear_Leakage,
	title = {Integrate-and-fire Models with Nonlinear Leakage},
	journal = {Bulletin of Mathematical Biology},
	volume = {62},
	number = {3},
	pages = {467-481},
	year = {2000},
	issn = {0092-8240},
	doi = {https://doi.org/10.1006/bulm.1999.0162},
	url = {https://www.sciencedirect.com/science/article/pii/S0092824099901623},
	author = {Jianfeng {Feng and David Brown}},
	abstract = {Can we express biophysical neuronal models as integrate-and-fire (IF) models with leakage coefficients which are no longer constant, as in the conventional leaky IF model, but functions of membrane potential and other biophysical variables? We illustrate the answer to this question using the FitzHugh–Nagumo (FHN) model as an example. A novel IF type model, the IF-FHN model, which approximates to the FHN model, is obtained. The leakage coefficient derived in the IF-FHN model has nonmonotonic relationship with membrane potential, revealing at least in part the intrinsic mechanisms underlying the FHN models. The IF-FHN model correspondingly exhibits more complex behaviour than the standard IF model. For example, in some parameter regions, the IF-FHN model has a coefficient of variation of the output interspike interval which is independent of the number of inhibitory inputs, being close to unity over the whole range, comparable to the FHN model as we noted previously (Brown et al., 1999).}
}

@article{1997-NeuralNetwork-Networks_of_spiking_neurons_The_third_generation_of_neural_network_models,
	title = {Networks of spiking neurons: The third generation of neural network models},
	journal = {Neural Networks},
	volume = {10},
	number = {9},
	pages = {1659-1671},
	year = {1997},
	issn = {0893-6080},
	doi = {https://doi.org/10.1016/S0893-6080(97)00011-7},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608097000117},
	author = {Wolfgang Maass},
	keywords = {Spiking neuron, Integrate-and-fire neutron, Computational complexity, Sigmoidal neural nets, Lower bounds},
	abstract = {The computational power of formal models for networks of spiking neurons is compared with that of other neural network models based on McCulloch Pitts neurons (i.e., threshold gates), respectively, sigmoidal gates. In particular it is shown that networks of spiking neurons are, with regard to the number of neurons that are needed, computationally more powerful than these other neural network models. A concrete biologically relevant function is exhibited which can be computed by a single spiking neuron (for biologically reasonable values of its parameters), but which requires hundreds of hidden units on a sigmoidal neural net. On the other hand, it is known that any function that can be computed by a small sigmoidal neural net can also be computed by a small network of spiking neurons. This article does not assume prior knowledge about spiking neurons, and it contains an extensive list of references to the currently available literature on computations in networks of spiking neurons and relevant results from neurobiology.}
}

@misc{2014-arxiv-Recurrent_Neural_Network_Regularization,
	title	= {Recurrent Neural Network Regularization},
	author	= {Wojciech Zaremba and Ilya Sutskever and Oriol Vinyals},
	year	= {2014},
	URL	= {https://arxiv.org/abs/1409.2329}
}

@inproceedings{2010-INTERSPEECH-Recurrent_neural_network_based_language_model,
	author = {Mikolov, Tomas and Karafiát, Martin and Burget, Lukas and Cernocký, Jan and Khudanpur, Sanjeev},
	year = {2010},
	month = {01},
	pages = {1045-1048},
	title = {Recurrent neural network based language model},
	volume = {2},
	journal = {Proceedings of the 11th Annual Conference of the International Speech Communication Association, INTERSPEECH 2010}
}

@book{1999-Book-Recurrent_Neural_Networks_Design_and_Applications,
	author = {Jain, L. C. and Medsker, L. R.},
	title = {Recurrent Neural Networks: Design and Applications},
	year = {1999},
	isbn = {0849371813},
	publisher = {CRC Press, Inc.},
	address = {USA},
	edition = {1st},
	abstract = {From the Publisher:With applications ranging from motion detection to financial forecasting, recurrent neural networks (RNNs) have emerged as an interesting and important part of neural network research. Recurrent Neural Networks: Design and Applications reflects the tremendous, worldwide interest in and virtually unlimited potential of RNNs - providing a summary of the design, applications, current research, and challenges of this dynamic and promising field.}
}

@ARTICLE{1997-IEEESP-Bidirectional_recurrent_neural_networks,  
	author={M. {Schuster} and K. K. {Paliwal}},  
	journal={IEEE Transactions on Signal Processing},   
	title={Bidirectional recurrent neural networks},   
	year={1997},  
	volume={45},  
	number={11},  
	pages={2673-2681},  
	doi={10.1109/78.650093}
}

@article{2009-Elsevier-Reservoir_computing_approaches_to_recurrent_neural_network_training,
	title = {Reservoir computing approaches to recurrent neural network training},
	journal = {Computer Science Review},
	volume = {3},
	number = {3},
	pages = {127-149},
	year = {2009},
	issn = {1574-0137},
	doi = {https://doi.org/10.1016/j.cosrev.2009.03.005},
	url = {https://www.sciencedirect.com/science/article/pii/S1574013709000173},
	author = {Mantas Lukoševičius and Herbert Jaeger},
	abstract = {Echo State Networks and Liquid State Machines introduced a new paradigm in artificial recurrent neural network (RNN) training, where an RNN (the reservoir) is generated randomly and only a readout is trained. The paradigm, becoming known as reservoir computing, greatly facilitated the practical application of RNNs and outperformed classical fully trained RNNs in many tasks. It has lately become a vivid research field with numerous extensions of the basic idea, including reservoir adaptation, thus broadening the initial paradigm to using different methods for training the reservoir and the readout. This review systematically surveys both current ways of generating/adapting the reservoirs and training different types of readouts. It offers a natural conceptual classification of the techniques, which transcends boundaries of the current “brand-names” of reservoir methods, and thus aims to help in unifying the field and providing the reader with a detailed “map” of it.}
}

@article{1987-Phys-Generalization_of_back_propagation_to_recurrent_neural_networks,
  title = {Generalization of back-propagation to recurrent neural networks},
  author = {Pineda, Fernando J.},
  journal = {Phys. Rev. Lett.},
  volume = {59},
  issue = {19},
  pages = {2229--2232},
  numpages = {0},
  year = {1987},
  month = {Nov},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.59.2229},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.59.2229}
}

@article{2021-SD-ASRNN__A_recurrent_neural_network_with_an_attention_model_for_sequence_labeling,
	title = {ASRNN: A recurrent neural network with an attention model for sequence labeling},
	journal = {Knowledge-Based Systems},
	volume = {212},
	pages = {106548},
	year = {2021},
	issn = {0950-7051},
	doi = {https://doi.org/10.1016/j.knosys.2020.106548},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120306778},
	author = {Jerry Chun-Wei Lin and Yinan Shao and Youcef Djenouri and Unil Yun},
	keywords = {Semi-CRF, Attention mechanism, Sequence labeling, Neural network},
	abstract = {Natural language processing (NLP) is useful for handling text and speech, and sequence labeling plays an important role by automatically analyzing a sequence (text) to assign category labels to each part. However, the performance of these conventional models depends greatly on hand-crafted features and task-specific knowledge, which is a time consuming task. Several conditional random fields (CRF)-based models for sequence labeling have been presented, but the major limitation is how to use neural networks for extracting useful representations for each unit or segment in the input sequence. In this paper, we propose an attention segmental recurrent neural network (ASRNN) that relies on a hierarchical attention neural semi-Markov conditional random fields (semi-CRF) model for the task of sequence labeling. Our model uses a hierarchical structure to incorporate character-level and word-level information and applies an attention mechanism to both levels. This enables our method to differentiate more important information from less important information when constructing the segmental representation. We evaluated our model on three sequence labeling tasks, including named entity recognition (NER), chunking, and reference parsing. Experimental results show that the proposed model benefited from the hierarchical structure, and it achieved competitive and robust performance on all three sequence labeling tasks.}
}  

@article{2020-Phys-Recurrent_neural_network_wave_functions,
  title = {Recurrent neural network wave functions},
  author = {Hibat-Allah, Mohamed and Ganahl, Martin and Hayward, Lauren E. and Melko, Roger G. and Carrasquilla, Juan},
  journal = {Phys. Rev. Research},
  volume = {2},
  issue = {2},
  pages = {023358},
  numpages = {17},
  year = {2020},
  month = {Jun},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevResearch.2.023358},
  url = {https://link.aps.org/doi/10.1103/PhysRevResearch.2.023358}
}

@article{2020-NN-Improved_recurrent_neural_network_based_manipulator_control_with_remote_center_of_motion_constraints__Experimental_results,
	title = {Improved recurrent neural network-based manipulator control with remote center of motion constraints: Experimental results},
	journal = {Neural Networks},
	volume = {131},
	pages = {291-299},
	year = {2020},
	issn = {0893-6080},
	doi = {https://doi.org/10.1016/j.neunet.2020.07.033},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608020302744},
	author = {Hang Su and Yingbai Hu and Hamid Reza Karimi and Alois Knoll and Giancarlo Ferrigno and Elena {De Momi}},
	keywords = {Recurrent neural network, Remote center of motion, Redundant manipulator, Robot-assisted minimally invasive surgery},
	abstract = {In this paper, an improved recurrent neural network (RNN) scheme is proposed to perform the trajectory control of redundant robot manipulators using remote center of motion (RCM) constraints. Firstly, learning by demonstration is implemented to model the surgical operation skills in the Cartesian space. After that, considering the kinematic constraints associated with the optimization control of redundant manipulators, we propose a novel RNN-based approach to facilitate accurate task tracking based on the general quadratic performance index, which includes managing the constraints on RCM joint angle, and joint velocity, simultaneously. The results of the conducted theoretical analysis confirm that the RCM constraint has been established successfully, and accordingly. The corresponding end-effector tracking errors asymptotically converge to zero. Finally, demonstration experiments are conducted in a laboratory setup environment using KUKA LWR4+ to validate the effectiveness of the proposed control strategy.}
}

@article{2020-Information-A_body_sensor_data_fusion_and_deep_recurrent_neural_network_based_behavior_recognition_approach_for_robust_healthcare,
	title = {A body sensor data fusion and deep recurrent neural network-based behavior recognition approach for robust healthcare},
	journal = {Information Fusion},
	volume = {55},
	pages = {105-115},
	year = {2020},
	issn = {1566-2535},
	doi = {https://doi.org/10.1016/j.inffus.2019.08.004},
	url = {https://www.sciencedirect.com/science/article/pii/S1566253519302581},
	author = {Md. Zia Uddin and Mohammed Mehedi Hassan and Ahmed Alsanad and Claudio Savaglio},
	keywords = {Body sensor data fusion, Behavior recognition, Deep recurrent neural network, Robust healthcare},
	abstract = {Recently, human healthcare from body sensor data has been getting remarkable research attentions by a huge range of human-computer interaction and pattern analysis researchers due to its practical applications such as smart health care systems. For example, smart wearable-based behavior recognition system can be used to assist the rehabilitation of patients in a smart clinic to improve the rehabilitation process and to prolong their independent life. Although there are many ways of using distributed sensors to monitor vital signs and behavior of people, physical human action recognition via body sensors provides valuable data regarding an individual's functionality and lifestyle. In this work, we propose a body sensor-based system for behavior recognition using deep Recurrent Neural Network (RNN), a promising deep learning algorithm based on sequential information. We perform data fusion from multiple body sensors such as electrocardiography (ECG), accelerometer, magnetometer, etc. The extracted features are further enhanced via kernel principal component analysis (KPCA). The robust features are then used to train an activity RNN, which is later used for behavior recognition. The system has been compared against the conventional approaches on three publicly available standard datasets. The experimental results show that the proposed approach outperforms the available state-of-the-art methods.}
}

@InProceedings{2018-CVPR-Independently_Recurrent_Neural_Network__IndRNN___Building_a_Longer_and_Deeper_RNN,
	author = {Li, Shuai and Li, Wanqing and Cook, Chris and Zhu, Ce and Gao, Yanbo},
	title = {Independently Recurrent Neural Network (IndRNN): Building a Longer and Deeper RNN},
	booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	month = {June},
	year = {2018}
} 

@inproceedings{2017-Computing-Sequential_User_Based_Recurrent_Neural_Network_Recommendations,
	author = {Donkers, Tim and Loepp, Benedikt and Ziegler, J\"{u}rgen},
	title = {Sequential User-Based Recurrent Neural Network Recommendations},
	year = {2017},
	isbn = {9781450346528},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3109859.3109877},
	doi = {10.1145/3109859.3109877},
	abstract = {Recurrent Neural Networks are powerful tools for modeling sequences. They are flexibly extensible and can incorporate various kinds of information including temporal order. These properties make them well suited for generating sequential recommendations. In this paper, we extend Recurrent Neural Networks by considering unique characteristics of the Recommender Systems domain. One of these characteristics is the explicit notion of the user recommendations are specifically generated for. We show how individual users can be represented in addition to sequences of consumed items in a new type of Gated Recurrent Unit to effectively produce personalized next item recommendations. Offline experiments on two real-world datasets indicate that our extensions clearly improve objective performance when compared to state-of-the-art recommender algorithms and to a conventional Recurrent Neural Network.},
	booktitle = {Proceedings of the Eleventh ACM Conference on Recommender Systems},
	pages = {152–160},
	numpages = {9},
	keywords = {deep learning, recommender systems, sequential recommendations, recurrent neural networks, neural networks},
	location = {Como, Italy},
	series = {RecSys '17}
}

@misc{2018-arxiv-Diffusion_Convolutional_Recurrent_Neural_Network__Data_Driven_Traffic_Forecasting,
      title={Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting}, 
      author={Yaguang Li and Rose Yu and Cyrus Shahabi and Yan Liu},
      year={2018},
      eprint={1707.01926},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{2020-Physica-Fundamentals_of_Recurrent_Neural_Network__RNN__and_Long_Short_Term_Memory__LSTM__network,
	title = {Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) network},
	journal = {Physica D: Nonlinear Phenomena},
	volume = {404},
	pages = {132306},
	year = {2020},
	issn = {0167-2789},
	doi = {https://doi.org/10.1016/j.physd.2019.132306},
	url = {https://www.sciencedirect.com/science/article/pii/S0167278919305974},
	author = {Alex Sherstinsky},
	keywords = {RNN, RNN unfolding/unrolling, LSTM, External input gate, Convolutional input context windows},
	abstract = {Because of their effectiveness in broad practical applications, LSTM networks have received a wealth of coverage in scientific journals, technical blogs, and implementation guides. However, in most articles, the inference formulas for the LSTM network and its parent, RNN, are stated axiomatically, while the training formulas are omitted altogether. In addition, the technique of “unrolling” an RNN is routinely presented without justification throughout the literature. The goal of this tutorial is to explain the essential RNN and LSTM fundamentals in a single document. Drawing from concepts in Signal Processing, we formally derive the canonical RNN formulation from differential equations. We then propose and prove a precise statement, which yields the RNN unrolling technique. We also review the difficulties with training the standard RNN and address them by transforming the RNN into the “Vanilla LSTM”1 1The nickname “Vanilla LSTM” symbolizes this model’s flexibility and generality (Greff et al., 2015). network through a series of logical arguments. We provide all equations pertaining to the LSTM system together with detailed descriptions of its constituent entities. Albeit unconventional, our choice of notation and the method for presenting the LSTM system emphasizes ease of understanding. As part of the analysis, we identify new opportunities to enrich the LSTM system and incorporate these extensions into the Vanilla LSTM network, producing the most general LSTM variant to date. The target reader has already been exposed to RNNs and LSTM networks through numerous available resources and is open to an alternative pedagogical approach. A Machine Learning practitioner seeking guidance for implementing our new augmented LSTM model in software for experimentation and research will find the insights and derivations in this treatise valuable as well.}
}

@article{2019-Smart_Grid-Short_Term_Residential_Load_Forecasting_Based_on_LSTM_Recurrent_Neural_Network,
  author={W. {Kong} and Z. Y. {Dong} and Y. {Jia} and D. J. {Hill} and Y. {Xu} and Y. {Zhang}},
  journal={IEEE Transactions on Smart Grid}, 
  title={Short-Term Residential Load Forecasting Based on LSTM Recurrent Neural Network}, 
  year={2019},
  volume={10},
  number={1},
  pages={841-851},
  doi={10.1109/TSG.2017.2753802}
}
 
@INPROCEEDINGS{1999-IJCNN-Real_time_short_term_natural_water_inflows_forecasting_using_recurrent_neural_networks,  
	author={P. {Coulibaly} and F. {Anctil}},  
	booktitle={IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339)},   
	title={Real-time short-term natural water inflows forecasting using recurrent neural networks},   
	year={1999},  
	volume={6},  
	number={},  
	pages={3802-3805 vol.6},  
	doi={10.1109/IJCNN.1999.830759}
} 

@INPROCEEDINGS{1999-IJCNN-A_new_recurrent_network_based_music_synthesis_method_for_Chinese_plucked_string_instruments___Pipa_and_Qin,  
	author={ {Sheng-Fu Liang} and A. W. Y. {Su} and  {Cheng-Teng Lin}},  
	booktitle={IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339)},   
	title={A new recurrent-network-based music synthesis method for Chinese plucked-string instruments - Pipa and Qin},   
	year={1999},  volume={4},  number={},  pages={2564-2569 vol.4},  doi={10.1109/IJCNN.1999.833478}
}

@article{1998-CIFER-Rule_Inference_for_Financial_Prediction_using_Recurrent_Neural_Networks,
	author = {Giles, C. and Lawrence, Steve and Tsoi, Ah},
	year = {1998},
	month = {01},
	pages = {},
	title = {Rule Inference for Financial Prediction using Recurrent Neural Networks},
	doi = {10.1109/CIFER.1997.618945}
}

@INPROCEEDINGS{2013-ICASSP-Speech_recognition_with_deep_recurrent_neural_networks,  
	author={A. {Graves} and A. {Mohamed} and G. {Hinton}},  
	booktitle={2013 IEEE International Conference on Acoustics, Speech and Signal Processing},   
	title={Speech recognition with deep recurrent neural networks},   
	year={2013},  volume={},  number={},  pages={6645-6649},  
	doi={10.1109/ICASSP.2013.6638947}
}

@misc{2014-arXiv-Long_Short_Term_Memory_Based_Recurrent_Neural_Network_Architectures_for_Large_Vocabulary_Speech_Recognition,
      title={Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition}, 
      author={Haşim Sak and Andrew Senior and Françoise Beaufays},
      year={2014},
      eprint={1402.1128},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@InProceedings{2014-ML-Towards_End_To_End_Speech_Recognition_with_Recurrent_Neural_Networks, 
	title = {Towards End-To-End Speech Recognition with Recurrent Neural Networks}, 
	author = {Alex Graves and Navdeep Jaitly}, 
	booktitle = {Proceedings of the 31st International Conference on Machine Learning}, 
	pages = {1764--1772}, year = {2014}, editor = {Eric P. Xing and Tony Jebara}, 
	volume = {32}, number = {2}, series = {Proceedings of Machine Learning Research}, 
	address = {Bejing, China}, month = {22--24 Jun}, publisher = {PMLR}, 
	pdf = {http://proceedings.mlr.press/v32/graves14.pdf}, url = {http://proceedings.mlr.press/v32/graves14.html}, 
	abstract = {This paper presents a speech recognition system that directly transcribes audio data with text, without requiring an intermediate phonetic representation. The system is based on a combination of the deep bidirectional LSTM recurrent neural network architecture and the Connectionist Temporal Classification objective function. A modification to the objective function is introduced that trains the network to minimise the expectation of an arbitrary transcription loss function. This allows a direct optimisation of the word error rate, even in the absence of a lexicon or language model. The system achieves a word error rate of 27.3% on the Wall Street Journal corpus with no prior linguistic information, 21.9% with only a lexicon of allowed words, and 8.2% with a trigram language model. Combining the network with a baseline system further reduces the error rate to 6.7%.} 
}

@misc{2015-arXiv-Fast_and_Accurate_Recurrent_Neural_Network_Acoustic_Models_for_Speech_Recognition,
      title={Fast and Accurate Recurrent Neural Network Acoustic Models for Speech Recognition}, 
      author={Haşim Sak and Andrew Senior and Kanishka Rao and Françoise Beaufays},
      year={2015},
      eprint={1507.06947},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{1986-RNN-Serial_order_a_parallel_distributed_processing_approach__Technical_report__June_1985_March_1986,
	title = {Serial order: a parallel distributed processing approach. Technical report, June 1985-March 1986},
	author = {Jordan, M I},
	abstractNote = {A theory of serial order is proposed that attempts to deal both with the classical problem of the temporal organization of internally generated action sequences as well as with certain of the parallel aspects of sequential behavior. The theory describes a dynamical system that is embodied as a parallel distributed processing or connectionist network. The trajectories of this dynamical system come to follow desired paths corresponding to particular action sequences as a result of a learning process during which constraints are imposed on the system. These constraints enforce sequentiality where necessary and, as they are relaxed, performance becomes more parallel. The theory is applied to the problem of coarticulation in speech production and simulation experiments are presented.},
	doi = {},
	url = {https://www.osti.gov/biblio/6910294}, journal = {},
	number = ,
	volume = ,
	place = {United States},
	year = {1986},
	month = {5}
} 

@inproceedings{1986-RNN-Learning_internal_representations_by_error_propagation,
  title={Learning internal representations by error propagation},
  author={D. Rumelhart and Geoffrey E. Hinton and R. J. Williams},
  year={1986}
} 

@inproceedings{2013-ML-On_the_difficulty_of_training_recurrent_neural_networks,
  title={On the difficulty of training recurrent neural networks},
  author={Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={1310--1318},
  year={2013},
  organization={PMLR}
} 

@article{1998-WorldScientific-The_vanishing_gradient_problem_during_learning_recurrent_neural_nets_and_problem_solutions,
  title={The vanishing gradient problem during learning recurrent neural nets and problem solutions},
  author={Hochreiter, Sepp},
  journal={International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
  volume={6},
  number={02},
  pages={107--116},
  year={1998},
  publisher={World Scientific}
}

@article{1994-NN-Learning_long_term_dependencies_with_gradient_descent_is_difficult,  
	author={Y. {Bengio} and P. {Simard} and P. {Frasconi}},  
	journal={IEEE Transactions on Neural Networks},   
	title={Learning long-term dependencies with gradient descent is difficult},   
	year={1994},  volume={5},  number={2},  pages={157-166},  
	doi={10.1109/72.279181}
}

@article{1997-MIT-Long_Short_Term_Memory,
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    title = "{Long Short-Term Memory}",
    journal = {Neural Computation},
    volume = {9},
    number = {8},
    pages = {1735-1780},
    year = {1997},
    month = {11},
    abstract = "{Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.8.1735},
    url = {https://doi.org/10.1162/neco.1997.9.8.1735},
    eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
} 
 
@article{1999-Elsevier-An_empirical_comparison_of_four_initialization_methods_for_the_k_means_algorithm,
  title={An empirical comparison of four initialization methods for the k-means algorithm},
  author={Pena, Jos{\'e} M and Lozano, Jose Antonio and Larranaga, Pedro},
  journal={Pattern recognition letters},
  volume={20},
  number={10},
  pages={1027--1040},
  year={1999},
  publisher={Elsevier}
} 

@article{2004-Elsevier-Cluster_center_initialization_algorithm_for_K_means_clustering,
  title={Cluster center initialization algorithm for K-means clustering},
  author={Khan, Shehroz S and Ahmad, Amir},
  journal={Pattern recognition letters},
  volume={25},
  number={11},
  pages={1293--1302},
  year={2004},
  publisher={Elsevier}
}

@article{2014-IJCP-EBK_means__A_clustering_technique_based_on_elbow_method_and_k_means_in_WSN,
  title={EBK-means: A clustering technique based on elbow method and k-means in WSN},
  author={Bholowalia, Purnima and Kumar, Arvind},
  journal={International Journal of Computer Applications},
  volume={105},
  number={9},
  year={2014},
  publisher={Citeseer}
}

@inproceedings{2018-IOP-Integration_k_means_clustering_method_and_elbow_method_for_identification_of_the_best_customer_profile_cluster,
  title={Integration k-means clustering method and elbow method for identification of the best customer profile cluster},
  author={Syakur, MA and Khotimah, BK and Rochman, EMS and Satoto, BD},
  booktitle={IOP Conference Series: Materials Science and Engineering},
  volume={336},
  number={1},
  pages={012017},
  year={2018},
  organization={IOP Publishing}
}
 